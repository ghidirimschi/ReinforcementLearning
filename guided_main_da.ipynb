{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guided_main_da.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train an agent to play StarPilot"
      ],
      "metadata": {
        "id": "XNDeePgxBasd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this jupyter notebook, we provide a stepwise procedure to train an agent to play StarPilot game using Procgen framework. Here, we used PPO that specifies how the agent should operate together with IMPALA neural network architecture. We also used UCB-DrAC algorithm to apply automatic data augmenation on the input. Thus, the objective function of PPO is slightly modified. The implementation is described in the report. Make sure to upload utils.py and data_augs.py on each session and to select GPU as the hardware accelerator."
      ],
      "metadata": {
        "id": "q44Xyco992JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Procgen framework to the current session:"
      ],
      "metadata": {
        "id": "a7UUBtO0_ytg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install procgen"
      ],
      "metadata": {
        "id": "ZOR58kWb_y-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Kornia to the current session:"
      ],
      "metadata": {
        "id": "DqA9Vc5USxNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "Vk4VggsCSz6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the hyperparameters:"
      ],
      "metadata": {
        "id": "1hJ1Je8t84il"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbzODSZg8Gmg"
      },
      "outputs": [],
      "source": [
        "total_steps = 1e5\n",
        "num_envs = 32\n",
        "num_levels = 200 # 10\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 128 #8 #512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "aug_coef = 0.1\n",
        "ucb_exploration_coef = 0.5\n",
        "total_num = 1\n",
        "ucb_window_length = 10\n",
        "num_interval = num_envs * num_steps\n",
        "grayscale = False\n",
        "feature_dim = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the relevant libraries:"
      ],
      "metadata": {
        "id": "loh7UopL9DwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "import data_augs\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Qm7Vh3nq8q2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the augmentation functions:"
      ],
      "metadata": {
        "id": "BlGtH9SLP5jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_to_func = {\n",
        "        'crop': data_augs.Crop,\n",
        "        'grayscale': data_augs.Grayscale,\n",
        "        'random-conv': data_augs.RandomConv,\n",
        "        'flip': data_augs.Flip,\n",
        "        'rotate': data_augs.Rotate,\n",
        "        'cutout': data_augs.Cutout,\n",
        "        'cutout-color': data_augs.CutoutColor,\n",
        "        'color-jitter': data_augs.ColorJitter,\n",
        "}"
      ],
      "metadata": {
        "id": "0AHrZWfvP8iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UCB policy to select the most appropriate augmentations:"
      ],
      "metadata": {
        "id": "V3u09yS6QBTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_augs = len(aug_to_func)\n",
        "expl_action = [0.] * number_of_augs\n",
        "ucb_action = [0.] * number_of_augs\n",
        "num_action = [1.] * number_of_augs\n",
        "qval_action = [0.] * number_of_augs\n",
        "return_action = []\n",
        "for i in range(number_of_augs):\n",
        "            return_action.append(deque(maxlen=ucb_window_length))\n",
        "\n",
        "aug_id = data_augs.Identity\n",
        "aug_list = [aug_to_func[t](batch_size=batch_size) for t in list(aug_to_func.keys())]\n",
        "\n",
        "def select_ucb_aug():\n",
        "    for i in range(number_of_augs):\n",
        "        expl_action[i] = ucb_exploration_coef * \\\n",
        "            np.sqrt(np.log(total_num) / num_action[i])\n",
        "        ucb_action[i] = qval_action[i] + expl_action[i]\n",
        "    ucb_aug_id = np.argmax(ucb_action)\n",
        "    return ucb_aug_id, aug_list[ucb_aug_id]"
      ],
      "metadata": {
        "id": "x0Z1dhD_RDFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes that represent the network architecture and the policy:"
      ],
      "metadata": {
        "id": "wexPi3GA9dLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x)\n",
        "        out = F.relu(self.conv1(out))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class ConvSequence(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(ConvSequence, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=(3,3), stride=2)\n",
        "        self.residual1 = Residual(depth, depth)\n",
        "        self.residual2 = Residual(depth, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = self.residual1(x)\n",
        "        x = self.residual2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ConvSequence(1 if grayscale else in_channels, 16),\n",
        "            ConvSequence(16, 32),\n",
        "            ConvSequence(32, 32),\n",
        "            Flatten(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1568, feature_dim)\n",
        "        )\n",
        "        self.apply(orthogonal_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if grayscale:\n",
        "            x = torchvision.transforms.Grayscale()(x)\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, encoder, feature_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            value, dist, _ = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "        return value, dist, logits\n",
        "\n",
        "    def get_value(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            value, _, _ = self.forward(x)\n",
        "\n",
        "        return value"
      ],
      "metadata": {
        "id": "QOhk3fWr9rgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the enviroment, network, optimizer, and storage and start training using PPO with DrAC: (All results will be saved into data.txt. A checkpoint will also be saved after training to eventually render a video of the agent playing the game.)"
      ],
      "metadata": {
        "id": "SqLUkQGxARrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define environment\n",
        "    # check the utils.py file for info on arguments\n",
        "    env = make_env(num_envs, num_levels=num_levels)\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "\n",
        "    # Define network\n",
        "    input_channels = env.observation_space.shape[0]\n",
        "    encoder = Encoder(input_channels, feature_dim)\n",
        "    policy = Policy(encoder, feature_dim, env.action_space.n)\n",
        "    policy.cuda()\n",
        "\n",
        "    # Define optimizer\n",
        "    # these are reasonable values but probably not optimal\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "    # Define temporary storage\n",
        "    # we use this to collect transitions during each iteration\n",
        "    storage = Storage(\n",
        "        env.observation_space.shape,\n",
        "        num_steps,\n",
        "        num_envs\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    obs = env.reset()\n",
        "    step = 0\n",
        "    max_mean = 0\n",
        "    while step < total_steps:\n",
        "\n",
        "        # open a file by creating it as text\n",
        "        f = open('data.txt','a')\n",
        "        \n",
        "        # Use policy to collect data for num_steps steps\n",
        "        policy.eval()\n",
        "        for i in range(num_steps):\n",
        "\n",
        "            # Use policy\n",
        "            action, log_prob, value = policy.act(obs)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store data\n",
        "            storage.store(next_obs, action, reward, done, info, log_prob, value)\n",
        "\n",
        "            # Update current observation\n",
        "            obs = aug_id(storage.obs[i])\n",
        "\n",
        "        # Add the last observation to collected data\n",
        "        obs = aug_id(storage.obs[-1])\n",
        "        _, _, value = policy.act(obs)\n",
        "        storage.store_last(obs, value)\n",
        "\n",
        "        # Compute return and advantage\n",
        "        storage.compute_return_advantage()\n",
        "\n",
        "        # Optimize policy\n",
        "        policy.train()\n",
        "\n",
        "        # Update UCB values\n",
        "        if step > 0:\n",
        "          total_num += num_interval\n",
        "          num_action[current_aug_id] += num_interval\n",
        "          return_action[current_aug_id].append(storage.returns.mean().item())\n",
        "          qval_action[current_aug_id] = np.mean(return_action[current_aug_id])\n",
        "\n",
        "        # Select an augmentation\n",
        "        current_aug_id, current_aug_func = select_ucb_aug() \n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            # Iterate over batches of transitions\n",
        "            generator = storage.get_generator(batch_size)\n",
        "            for batch in generator:\n",
        "                b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "                # Get current policy outputs\n",
        "                new_value, new_dist, _ = policy(b_obs)\n",
        "                new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "                # Clipped policy objective\n",
        "                ratio = torch.exp(new_log_prob - b_log_prob)\n",
        "                clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)\n",
        "                policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n",
        "                pi_loss = -policy_reward.mean()\n",
        "\n",
        "                # Clipped value function objective\n",
        "                clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)\n",
        "                vf_loss = torch.max((new_value - b_returns) ** 2, (clipped_value - b_returns) ** 2)\n",
        "                value_loss = 0.5 * vf_loss.mean()\n",
        "\n",
        "                # Entropy loss\n",
        "                entropy_loss = new_dist.entropy()\n",
        "                entropy_loss = entropy_loss.mean()\n",
        "\n",
        "                # Augmentation loss\n",
        "                b_obs_aug = current_aug_func.do_augmentation(b_obs)\n",
        "                b_obs_id = aug_id(b_obs)\n",
        "                value_aug, dist_aug, _ = policy(b_obs_aug)\n",
        "                action_loss_aug = - dist_aug.log_prob(dist_aug.sample()).mean()\n",
        "                value_loss_aug = 0.5 * ((torch.detach(new_value) - value_aug) ** 2).mean()\n",
        "               \n",
        "                aug_loss = value_loss_aug + action_loss_aug\n",
        "\n",
        "                # Backpropagate losses\n",
        "                # loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss)\n",
        "                loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss + aug_loss * aug_coef)\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "                # Update policy\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                current_aug_func.change_randomization_params_all()\n",
        "\n",
        "        # Update stats\n",
        "        step += num_interval\n",
        "        print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "        # write to the file\n",
        "        f.write(f'Step: {step}\\tMean reward: {storage.get_reward()}\\n')\n",
        "        # close the file\n",
        "        f.close()\n",
        "        if storage.get_reward() > max_mean:\n",
        "            print('New high mean. Updating...')\n",
        "            torch.save(policy.state_dict(), 'checkpoint.pt')\n",
        "            max_mean = storage.get_reward()\n",
        "\n",
        "    print('Completed training!')"
      ],
      "metadata": {
        "id": "qb6Jchd4BHDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results:"
      ],
      "metadata": {
        "id": "5nXyd9Y6CN1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "filename = open('data.txt')\n",
        "l =  filename.readlines()\n",
        "d = [e.split() for e in l]\n",
        "\n",
        "dlen = len(d)\n",
        "X = np.zeros(dlen)\n",
        "Y = np.zeros(dlen)\n",
        "\n",
        "for i in range(dlen):\n",
        "    X[i] = d[i][1]\n",
        "    Y[i] = d[i][4]\n",
        "\n",
        "n = 20  # the larger n is, the smoother curve will be\n",
        "b = [1.0 / n] * n\n",
        "a = 1\n",
        "YY = lfilter(b,a,Y)    \n",
        "    \n",
        "plt.plot(X, YY)\n",
        "plt.legend()\n",
        "plt.locator_params(axis=\"x\", nbins=4)\n",
        "plt.locator_params(axis=\"y\", nbins=2)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Mean reward\")\n",
        "plt.savefig('Figure_1', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFHVe6JPCeDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the policy and save a video of a gameplay:"
      ],
      "metadata": {
        "id": "GOmZP_QUFsyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "nl = input('Enter a level: ')\n",
        "num_levels = int(nl)\n",
        "while True:\n",
        "\tub = input('Do you want to use backgrounds? (Y/N) ')\n",
        "\tif ub == 'Y':\n",
        "\t\tuse_backgrounds = True\n",
        "\t\tbreak\n",
        "\telif ub == 'N':\n",
        "\t\tuse_backgrounds = False\n",
        "\t\tbreak\n",
        "\telse:\n",
        "\t\tprint('Invalid input!')\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, use_backgrounds=use_backgrounds) #added distribution_mode\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "\n",
        "\n",
        "print(\"Generating video...\")\n",
        "for _ in range(512):\n",
        "\n",
        "  # Use policy\n",
        "  action, log_prob, value = policy.act(obs)\n",
        "\n",
        "  # Take step in environment\n",
        "  obs, reward, done, info = eval_env.step(action)\n",
        "  total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "  # Render environment and store\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "  frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=30)\n",
        "print('Video saved!')"
      ],
      "metadata": {
        "id": "68OtSnMfGQgs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}