{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guided_main_da.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train an agent to play StarPilot"
      ],
      "metadata": {
        "id": "XNDeePgxBasd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this jupyter notebook, we provide a stepwise procedure to train an agent to play StarPilot game using Procgen framework. Here, we used PPO that specifies how the agent should operate together with IMPALA neural network architecture. We also used UCB-DrAC algorithm to apply automatic data augmenation on the input. Thus, the objective function of PPO is slightly modified. The implementation is described in the report. Make sure to upload utils.py and data_augs.py on each session and to select GPU as the hardware accelerator."
      ],
      "metadata": {
        "id": "q44Xyco992JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Procgen framework to the current session:"
      ],
      "metadata": {
        "id": "a7UUBtO0_ytg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install procgen"
      ],
      "metadata": {
        "id": "ZOR58kWb_y-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "e80e6d7c-6f4d-4e1b-e1c1-7af3b787f9e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting procgen\n",
            "  Downloading procgen-0.10.4-cp37-cp37m-manylinux2010_x86_64.whl (39.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 39.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (0.17.3)\n",
            "Collecting gym3<1.0.0,>=0.3.3\n",
            "  Downloading gym3-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (1.19.5)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (3.4.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.15.0)\n",
            "Collecting imageio-ffmpeg<0.4.0,>=0.3.0\n",
            "  Downloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 65.5 MB/s \n",
            "\u001b[?25hCollecting moderngl<6.0.0,>=5.5.4\n",
            "  Downloading moderngl-5.6.4-cp37-cp37m-manylinux1_x86_64.whl (670 kB)\n",
            "\u001b[K     |████████████████████████████████| 670 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting imageio<3.0.0,>=2.6.0\n",
            "  Downloading imageio-2.13.5-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting glfw<2.0.0,>=1.8.6\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[K     |████████████████████████████████| 203 kB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.21)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 44.1 MB/s \n",
            "\u001b[?25hCollecting glcontext<3,>=2\n",
            "  Downloading glcontext-2.3.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n",
            "Installing collected packages: pillow, glcontext, moderngl, imageio-ffmpeg, imageio, glfw, gym3, procgen\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed glcontext-2.3.4 glfw-1.12.0 gym3-0.3.3 imageio-2.13.5 imageio-ffmpeg-0.3.0 moderngl-5.6.4 pillow-9.0.0 procgen-0.10.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Kornia to the current session:"
      ],
      "metadata": {
        "id": "DqA9Vc5USxNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk4VggsCSz6D",
        "outputId": "3887cf9a-330d-4256-b37f-80330ca60a86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.6.2-py2.py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.6)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the hyperparameters:"
      ],
      "metadata": {
        "id": "1hJ1Je8t84il"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JbzODSZg8Gmg"
      },
      "outputs": [],
      "source": [
        "total_steps = 1e5\n",
        "num_envs = 32\n",
        "num_levels = 200 # 10\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 128 #8 #512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "aug_coef = 0.1\n",
        "ucb_exploration_coef = 0.5\n",
        "total_num = 1\n",
        "ucb_window_length = 10\n",
        "num_interval = num_envs * num_steps\n",
        "grayscale = False\n",
        "feature_dim = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the relevant libraries:"
      ],
      "metadata": {
        "id": "loh7UopL9DwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "import data_augs\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Qm7Vh3nq8q2W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the augmentation functions:"
      ],
      "metadata": {
        "id": "BlGtH9SLP5jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_to_func = {\n",
        "        'crop': data_augs.Crop,\n",
        "        'grayscale': data_augs.Grayscale,\n",
        "        'random-conv': data_augs.RandomConv,\n",
        "        'flip': data_augs.Flip,\n",
        "        'rotate': data_augs.Rotate,\n",
        "        'cutout': data_augs.Cutout,\n",
        "        'cutout-color': data_augs.CutoutColor,\n",
        "        'color-jitter': data_augs.ColorJitter,\n",
        "}"
      ],
      "metadata": {
        "id": "0AHrZWfvP8iF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UCB policy to select the most appropriate augmentations:"
      ],
      "metadata": {
        "id": "V3u09yS6QBTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_augs = len(aug_to_func)\n",
        "expl_action = [0.] * number_of_augs\n",
        "ucb_action = [0.] * number_of_augs\n",
        "num_action = [1.] * number_of_augs\n",
        "qval_action = [0.] * number_of_augs\n",
        "return_action = []\n",
        "for i in range(number_of_augs):\n",
        "            return_action.append(deque(maxlen=ucb_window_length))\n",
        "\n",
        "aug_id = data_augs.Identity\n",
        "aug_list = [aug_to_func[t](batch_size=batch_size) for t in list(aug_to_func.keys())]\n",
        "\n",
        "def select_ucb_aug():\n",
        "    for i in range(number_of_augs):\n",
        "        expl_action[i] = ucb_exploration_coef * \\\n",
        "            np.sqrt(np.log(total_num) / num_action[i])\n",
        "        ucb_action[i] = qval_action[i] + expl_action[i]\n",
        "    ucb_aug_id = np.argmax(ucb_action)\n",
        "    return ucb_aug_id, aug_list[ucb_aug_id]"
      ],
      "metadata": {
        "id": "x0Z1dhD_RDFz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes that represent the network architecture and the policy:"
      ],
      "metadata": {
        "id": "wexPi3GA9dLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x)\n",
        "        out = F.relu(self.conv1(out))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class ConvSequence(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(ConvSequence, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=(3,3), stride=2)\n",
        "        self.residual1 = Residual(depth, depth)\n",
        "        self.residual2 = Residual(depth, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = self.residual1(x)\n",
        "        x = self.residual2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ConvSequence(1 if grayscale else in_channels, 16),\n",
        "            ConvSequence(16, 32),\n",
        "            ConvSequence(32, 32),\n",
        "            Flatten(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1568, feature_dim)\n",
        "        )\n",
        "        self.apply(orthogonal_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if grayscale:\n",
        "            x = torchvision.transforms.Grayscale()(x)\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, encoder, feature_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            value, dist, _ = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "        return value, dist, logits\n",
        "\n",
        "    def get_value(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            value, _, _ = self.forward(x)\n",
        "\n",
        "        return value"
      ],
      "metadata": {
        "id": "QOhk3fWr9rgn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the enviroment, network, optimizer, and storage and start training using PPO with DrAC: (All results will be saved into data.txt. A checkpoint will also be saved after training to eventually render a video of the agent playing the game.)"
      ],
      "metadata": {
        "id": "SqLUkQGxARrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define environment\n",
        "    # check the utils.py file for info on arguments\n",
        "    env = make_env(num_envs, num_levels=num_levels)\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "\n",
        "    # Define network\n",
        "    input_channels = env.observation_space.shape[0]\n",
        "    encoder = Encoder(input_channels, feature_dim)\n",
        "    policy = Policy(encoder, feature_dim, env.action_space.n)\n",
        "    policy.cuda()\n",
        "\n",
        "    # Define optimizer\n",
        "    # these are reasonable values but probably not optimal\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "    # Define temporary storage\n",
        "    # we use this to collect transitions during each iteration\n",
        "    storage = Storage(\n",
        "        env.observation_space.shape,\n",
        "        num_steps,\n",
        "        num_envs\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    obs = env.reset()\n",
        "    step = 0\n",
        "    max_mean = 0\n",
        "    while step < total_steps:\n",
        "\n",
        "        # open a file by creating it as text\n",
        "        f = open('data.txt','a')\n",
        "        \n",
        "        # Use policy to collect data for num_steps steps\n",
        "        policy.eval()\n",
        "        for i in range(num_steps):\n",
        "\n",
        "            # Use policy\n",
        "            action, log_prob, value = policy.act(obs)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store data\n",
        "            storage.store(next_obs, action, reward, done, info, log_prob, value)\n",
        "\n",
        "            # Update current observation\n",
        "            obs = aug_id(storage.obs[i])\n",
        "\n",
        "        # Add the last observation to collected data\n",
        "        obs = aug_id(storage.obs[-1])\n",
        "        _, _, value = policy.act(obs)\n",
        "        storage.store_last(obs, value)\n",
        "\n",
        "        # Compute return and advantage\n",
        "        storage.compute_return_advantage()\n",
        "\n",
        "        # Optimize policy\n",
        "        policy.train()\n",
        "\n",
        "        # Update UCB values\n",
        "        if step > 0:\n",
        "          total_num += num_interval\n",
        "          num_action[current_aug_id] += num_interval\n",
        "          return_action[current_aug_id].append(storage.returns.mean().item())\n",
        "          qval_action[current_aug_id] = np.mean(return_action[current_aug_id])\n",
        "\n",
        "        # Select an augmentation\n",
        "        current_aug_id, current_aug_func = select_ucb_aug() \n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            # Iterate over batches of transitions\n",
        "            generator = storage.get_generator(batch_size)\n",
        "            for batch in generator:\n",
        "                b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "                # Get current policy outputs\n",
        "                new_value, new_dist, _ = policy(b_obs)\n",
        "                new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "                # Clipped policy objective\n",
        "                ratio = torch.exp(new_log_prob - b_log_prob)\n",
        "                clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)\n",
        "                policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n",
        "                pi_loss = -policy_reward.mean()\n",
        "\n",
        "                # Clipped value function objective\n",
        "                clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)\n",
        "                vf_loss = torch.max((new_value - b_returns) ** 2, (clipped_value - b_returns) ** 2)\n",
        "                value_loss = 0.5 * vf_loss.mean()\n",
        "\n",
        "                # Entropy loss\n",
        "                entropy_loss = new_dist.entropy()\n",
        "                entropy_loss = entropy_loss.mean()\n",
        "\n",
        "                # Augmentation loss\n",
        "                b_obs_aug = current_aug_func.do_augmentation(b_obs)\n",
        "                b_obs_id = aug_id(b_obs)\n",
        "                value_aug, dist_aug, _ = policy(b_obs_aug)\n",
        "                action_loss_aug = - dist_aug.log_prob(dist_aug.sample()).mean()\n",
        "                value_loss_aug = 0.5 * ((torch.detach(new_value) - value_aug) ** 2).mean()\n",
        "               \n",
        "                aug_loss = value_loss_aug + action_loss_aug\n",
        "\n",
        "                # Backpropagate losses\n",
        "                # loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss)\n",
        "                loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss + aug_loss * aug_coef)\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "                # Update policy\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                current_aug_func.change_randomization_params_all()\n",
        "\n",
        "        # Update stats\n",
        "        step += num_interval\n",
        "        print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "        # write to the file\n",
        "        f.write(f'Step: {step}\\tMean reward: {storage.get_reward()}\\n')\n",
        "        # close the file\n",
        "        f.close()\n",
        "        if storage.get_reward() > max_mean:\n",
        "            print('New high mean. Updating...')\n",
        "            torch.save(policy.state_dict(), 'checkpoint.pt')\n",
        "            max_mean = storage.get_reward()\n",
        "\n",
        "    print('Completed training!')"
      ],
      "metadata": {
        "id": "qb6Jchd4BHDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results:"
      ],
      "metadata": {
        "id": "5nXyd9Y6CN1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "filename = open('data.txt')\n",
        "l =  filename.readlines()\n",
        "d = [e.split() for e in l]\n",
        "\n",
        "dlen = len(d)\n",
        "X = np.zeros(dlen)\n",
        "Y = np.zeros(dlen)\n",
        "\n",
        "for i in range(dlen):\n",
        "    X[i] = d[i][1]\n",
        "    Y[i] = d[i][4]\n",
        "\n",
        "n = 20  # the larger n is, the smoother curve will be\n",
        "b = [1.0 / n] * n\n",
        "a = 1\n",
        "YY = lfilter(b,a,Y)    \n",
        "    \n",
        "plt.plot(X, YY)\n",
        "plt.legend()\n",
        "plt.locator_params(axis=\"x\", nbins=4)\n",
        "plt.locator_params(axis=\"y\", nbins=2)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Mean reward\")\n",
        "plt.savefig('Figure_1', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFHVe6JPCeDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the policy and save a video of a gameplay:"
      ],
      "metadata": {
        "id": "GOmZP_QUFsyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "nl = input('Enter a level: ')\n",
        "num_levels = int(nl)\n",
        "while True:\n",
        "\tub = input('Do you want to use backgrounds? (Y/N) ')\n",
        "\tif ub == 'Y':\n",
        "\t\tuse_backgrounds = True\n",
        "\t\tbreak\n",
        "\telif ub == 'N':\n",
        "\t\tuse_backgrounds = False\n",
        "\t\tbreak\n",
        "\telse:\n",
        "\t\tprint('Invalid input!')\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, use_backgrounds=use_backgrounds) #added distribution_mode\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "\n",
        "\n",
        "print(\"Generating video...\")\n",
        "for _ in range(512):\n",
        "\n",
        "  # Use policy\n",
        "  action, log_prob, value = policy.act(obs)\n",
        "\n",
        "  # Take step in environment\n",
        "  obs, reward, done, info = eval_env.step(action)\n",
        "  total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "  # Render environment and store\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "  frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=30)\n",
        "print('Video saved!')"
      ],
      "metadata": {
        "id": "68OtSnMfGQgs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}