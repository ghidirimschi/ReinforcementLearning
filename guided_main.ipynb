{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guided_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train an agent to play StarPilot"
      ],
      "metadata": {
        "id": "XNDeePgxBasd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this jupyter notebook, we provide a stepwise procedure to train an agent to play StarPilot game using Procgen framework. Here, we used PPO that specifies how the agent should operate together with IMPALA neural network architecture. The input is grayscaled. The implementation is described in the report. Make sure to upload utils.py on each session and to select GPU as the hardware accelerator."
      ],
      "metadata": {
        "id": "q44Xyco992JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Procgen framework to the current session:"
      ],
      "metadata": {
        "id": "a7UUBtO0_ytg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install procgen"
      ],
      "metadata": {
        "id": "ZOR58kWb_y-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the hyperparameters:"
      ],
      "metadata": {
        "id": "1hJ1Je8t84il"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JbzODSZg8Gmg"
      },
      "outputs": [],
      "source": [
        "total_steps = 32e6\n",
        "num_envs = 32\n",
        "num_levels = 200 # 10\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 128 #8 #512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "grayscale = True\n",
        "feature_dim = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the relevant libraries:"
      ],
      "metadata": {
        "id": "loh7UopL9DwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init"
      ],
      "metadata": {
        "id": "Qm7Vh3nq8q2W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes that represent the network architecture and the policy:"
      ],
      "metadata": {
        "id": "wexPi3GA9dLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x)\n",
        "        out = F.relu(self.conv1(out))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class ConvSequence(nn.Module):\n",
        "    def __init__(self, in_channels, depth):\n",
        "        super(ConvSequence, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, depth, 3, padding=1)\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=(3,3), stride=2)\n",
        "        self.residual1 = Residual(depth, depth)\n",
        "        self.residual2 = Residual(depth, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = self.residual1(x)\n",
        "        x = self.residual2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ConvSequence(1 if grayscale else in_channels, 16),\n",
        "            ConvSequence(16, 32),\n",
        "            ConvSequence(32, 32),\n",
        "            Flatten(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1568, feature_dim)\n",
        "        )\n",
        "        self.apply(orthogonal_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if grayscale:\n",
        "            x = torchvision.transforms.Grayscale()(x)\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, encoder, feature_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            dist, value = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "        return dist, value"
      ],
      "metadata": {
        "id": "QOhk3fWr9rgn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the enviroment, network, optimizer, and storage and start training using PPO: (All results will be saved into data.txt. A checkpoint will also be saved after training to eventually render a video of the agent playing the game.)"
      ],
      "metadata": {
        "id": "SqLUkQGxARrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define environment\n",
        "    # check the utils.py file for info on arguments\n",
        "    env = make_env(num_envs, num_levels=num_levels)\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "\n",
        "    # Define network\n",
        "    input_channels = env.observation_space.shape[0]\n",
        "    encoder = Encoder(input_channels, feature_dim)\n",
        "    policy = Policy(encoder, feature_dim, env.action_space.n)\n",
        "    policy.cuda()\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "    # Define temporary storage\n",
        "    # we use this to collect transitions during each iteration\n",
        "    storage = Storage(\n",
        "        env.observation_space.shape,\n",
        "        num_steps,\n",
        "        num_envs\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    obs = env.reset()\n",
        "    step = 0\n",
        "    max_mean = 0\n",
        "    while step < total_steps:\n",
        "\n",
        "        # open a file by creating it as text\n",
        "        f = open('data.txt','a')\n",
        "        \n",
        "        # Use policy to collect data for num_steps steps\n",
        "        policy.eval()\n",
        "        for _ in range(num_steps):\n",
        "            # Use policy\n",
        "            action, log_prob, value = policy.act(obs)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store data\n",
        "            storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "\n",
        "            # Update current observation\n",
        "            obs = next_obs\n",
        "\n",
        "        # Add the last observation to collected data\n",
        "        _, _, value = policy.act(obs)\n",
        "        storage.store_last(obs, value)\n",
        "\n",
        "        # Compute return and advantage\n",
        "        storage.compute_return_advantage()\n",
        "\n",
        "        # Optimize policy\n",
        "        policy.train()\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            # Iterate over batches of transitions\n",
        "            generator = storage.get_generator(batch_size)\n",
        "            for batch in generator:\n",
        "                b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "                # Get current policy outputs\n",
        "                new_dist, new_value = policy(b_obs)\n",
        "                new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "                # Clipped policy objective\n",
        "                ratio = torch.exp(new_log_prob - b_log_prob)\n",
        "                clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)\n",
        "                policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n",
        "                pi_loss = -policy_reward.mean()\n",
        "\n",
        "                # Clipped value function objective\n",
        "                clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)\n",
        "                vf_loss = torch.max((new_value - b_returns) ** 2, (clipped_value - b_returns) ** 2)\n",
        "                value_loss = 0.5 * vf_loss.mean()\n",
        "\n",
        "                # Entropy loss\n",
        "                entropy_loss = new_dist.entropy()\n",
        "                entropy_loss = entropy_loss.mean()\n",
        "\n",
        "                # Backpropagate losses\n",
        "                loss = (pi_loss + value_coef * value_loss - entropy_coef * entropy_loss)\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "                # Update policy\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        # Update stats\n",
        "        step += num_envs * num_steps\n",
        "        print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "        # write to the file\n",
        "        f.write(f'Step: {step}\\tMean reward: {storage.get_reward()}\\n')\n",
        "        # close the file\n",
        "        f.close()\n",
        "        if storage.get_reward() > max_mean:\n",
        "            print('New high mean. Updating...')\n",
        "            torch.save(policy.state_dict(), 'checkpoint.pt')\n",
        "            max_mean = storage.get_reward()\n",
        "\n",
        "    print('Completed training!')"
      ],
      "metadata": {
        "id": "qb6Jchd4BHDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results:"
      ],
      "metadata": {
        "id": "5nXyd9Y6CN1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "filename = open('data.txt')\n",
        "l =  filename.readlines()\n",
        "d = [e.split() for e in l]\n",
        "\n",
        "dlen = len(d)\n",
        "X = np.zeros(dlen)\n",
        "Y = np.zeros(dlen)\n",
        "\n",
        "for i in range(dlen):\n",
        "    X[i] = d[i][1]\n",
        "    Y[i] = d[i][4]\n",
        "\n",
        "n = 20  # the larger n is, the smoother curve will be\n",
        "b = [1.0 / n] * n\n",
        "a = 1\n",
        "YY = lfilter(b,a,Y)    \n",
        "    \n",
        "plt.plot(X, YY)\n",
        "plt.legend()\n",
        "plt.locator_params(axis=\"x\", nbins=4)\n",
        "plt.locator_params(axis=\"y\", nbins=2)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Mean reward\")\n",
        "plt.savefig('Figure_1', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFHVe6JPCeDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the policy and save a video of a gameplay:"
      ],
      "metadata": {
        "id": "GOmZP_QUFsyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "nl = input('Enter a level: ')\n",
        "num_levels = int(nl)\n",
        "while True:\n",
        "\tub = input('Do you want to use backgrounds? (Y/N) ')\n",
        "\tif ub == 'Y':\n",
        "\t\tuse_backgrounds = True\n",
        "\t\tbreak\n",
        "\telif ub == 'N':\n",
        "\t\tuse_backgrounds = False\n",
        "\t\tbreak\n",
        "\telse:\n",
        "\t\tprint('Invalid input!')\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, use_backgrounds=use_backgrounds) #added distribution_mode\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "\n",
        "\n",
        "print(\"Generating video...\")\n",
        "for _ in range(512):\n",
        "\n",
        "  # Use policy\n",
        "  action, log_prob, value = policy.act(obs)\n",
        "\n",
        "  # Take step in environment\n",
        "  obs, reward, done, info = eval_env.step(action)\n",
        "  total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "  # Render environment and store\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "  frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=30)\n",
        "print('Video saved!')"
      ],
      "metadata": {
        "id": "68OtSnMfGQgs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}